# Copyright 2024 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
import asyncio
import ast
import json
import logging
import os
from abc import ABC, abstractmethod

import regex as re
from pydantic import BaseModel

from verl.utils.rollout_trace import rollout_trace_op

logger = logging.getLogger(__file__)
logger.setLevel(os.getenv("VERL_LOGGING_LEVEL", "WARN"))


class FunctionCall(BaseModel):
    arguments: str
    """
    The arguments to call the function with, as generated by the model in JSON
    format. Note that the model does not always generate valid JSON, and may
    hallucinate parameters not defined by your function schema. Validate the
    arguments in your code before calling your function.
    """

    name: str
    """The name of the function to call."""


class ToolParser(ABC):
    _registry: dict[str, type["ToolParser"]] = {}

    def __init__(self, tokenizer) -> None:
        self.tokenizer = tokenizer

    @abstractmethod
    async def extract_tool_calls(self, responses_ids: list[int]) -> tuple[str, list[FunctionCall]]:
        """Extract tool calls from the responses.

        Args:
            responses_ids (List[int]): The ids of the responses.

        Returns:
            Tuple[str, List[FunctionCall]]: Content and extracted tool calls.
        """
        raise NotImplementedError

    @classmethod
    def get_tool_parser(cls, name: str, tokenizer):
        if name not in cls._registry:
            raise ValueError(f"Unknown tool parser: {name}")
        return cls._registry[name](tokenizer)

    @classmethod
    def register(cls, name: str):
        def decorator(subclass: type[ToolParser]) -> type[ToolParser]:
            cls._registry[name] = subclass
            return subclass

        return decorator


@ToolParser.register("hermes")
class HermesToolParser(ToolParser):
    """Adapted from https://github.com/vllm-project/vllm/blob/v0.9.1/vllm/entrypoints/openai/tool_parsers/hermes_tool_parser.py"""

    def __init__(self, tokenizer) -> None:
        super().__init__(tokenizer)

        self.tool_call_start_token: str = "<tool_call>"
        self.tool_call_end_token: str = "</tool_call>"
        self.tool_call_regex = re.compile(r"<tool_call>(.*?)</tool_call>", re.DOTALL)

    def _sanitize_json_block(self, block: str) -> str:
        """Best-effort cleaning before json.loads.

        - Strip whitespace
        - Remove BOM/zero-width/bidi control chars that models sometimes emit
        - Strip markdown fences like ```json ... ```
        - Keep only substring between the first '{' and the last '}'
        """
        s = block.strip()

        # Remove BOM and common zero-width / bidi marks
        try:
            s = s.lstrip("\ufeff")
        except Exception:
            pass
        s = re.sub(r"[\u200b-\u200f\ufeff\u202a-\u202e]", "", s)

        # Strip simple markdown code fences
        if s.startswith("```") and s.endswith("```"):
            s = s.strip("`")
            if s.lower().startswith("json"):
                s = s[4:].lstrip()

        # Keep only the JSON object bounds to drop any leading noise
        start = s.find("{")
        end = s.rfind("}")
        if start != -1 and end != -1 and end > start:
            s = s[start : end + 1]

        return s

    @rollout_trace_op
    async def extract_tool_calls(self, responses_ids: list[int]) -> tuple[str, list[FunctionCall]]:
        loop = asyncio.get_running_loop()
        text = await loop.run_in_executor(None, self.tokenizer.decode, responses_ids)
        if self.tool_call_start_token not in text or self.tool_call_end_token not in text:
            return text, []

        matches = self.tool_call_regex.findall(text)
        function_calls = []
        function_call = None
        for match in matches:
            try:
                cleaned = self._sanitize_json_block(match)
                function_call = json.loads(cleaned)
                name, arguments = function_call["name"], function_call["arguments"]
                function_calls.append(FunctionCall(name=name, arguments=json.dumps(arguments, ensure_ascii=False)))
            except Exception as e:
                # Use repr to surface hidden characters in logs
                logger.error(f"Failed to decode tool call: {e}" + "\nmatch: \n" + repr(match))


        # remaing text exclude tool call tokens
        content = self.tool_call_regex.sub("", text)

        return content, function_calls


@ToolParser.register("qwen3coder")
class QwenCoderToolParser(ToolParser):
    """Parser for Qwen3-Coder tool calls.
    
    Supports patterns like:
    TODO:

    """
    def __init__(self, tokenizer) -> None:
        super().__init__(tokenizer)

        # Tokens / regex adapted from vLLM Qwen3-Coder style
        self.tool_call_start_token: str = "<tool_call>"
        self.tool_call_end_token: str = "</tool_call>"
        self.tool_call_prefix: str = "<function="
        self.function_end_token: str = "</function>"
        self.parameter_prefix: str = "<parameter="
        self.parameter_end_token: str = "</parameter>"

        # Regex patterns
        self.tool_call_complete_regex = re.compile(r"<tool_call>(.*?)</tool_call>", re.DOTALL)
        self.tool_call_regex = re.compile(r"<tool_call>(.*?)</tool_call>|<tool_call>(.*?)$", re.DOTALL)
        self.tool_call_function_regex = re.compile(r"<function=(.*?)</function>|<function=(.*)$", re.DOTALL)
        self.tool_call_parameter_regex = re.compile(
            r"<parameter=(.*?)(?:</parameter>|(?=<parameter=)|(?=</function>)|$)",
            re.DOTALL,
        )

    def _convert_param_value(self, raw_value: str):
        """Best-effort conversion of parameter values without schema.

        - null -> None
        - true/false -> bool
        - int/float numbers -> numeric
        - JSON / Python-literal containers -> parsed object
        - otherwise keep as string
        """
        if raw_value is None:
            return None

        value = raw_value.strip()
        if value.lower() == "null":
            return None

        # Booleans
        lower = value.lower()
        if lower == "true":
            return True
        if lower == "false":
            return False

        # Numbers
        try:
            if re.fullmatch(r"[-+]?\d+", value):
                return int(value)
            if re.fullmatch(r"[-+]?(?:\d*\.\d+|\d+\.\d*)(?:[eE][-+]?\d+)?", value):
                f = float(value)
                return int(f) if f.is_integer() else f
        except Exception:
            pass

        # Try JSON first
        try:
            return json.loads(value)
        except Exception:
            pass

        # Fallback to Python literal (safer than eval)
        try:
            return ast.literal_eval(value)
        except Exception:
            pass

        return raw_value

    def _parse_xml_function_call(self, function_call_str: str) -> FunctionCall | None:
        # Extract function name
        if ">" not in function_call_str:
            return None
        end_index = function_call_str.index(">")
        function_name = function_call_str[:end_index]
        parameters = function_call_str[end_index + 1 :]

        param_dict: dict[str, object] = {}
        for match_text in self.tool_call_parameter_regex.findall(parameters):
            if ">" not in match_text:
                # malformed parameter, skip
                continue
            idx = match_text.index(">")
            param_name = match_text[:idx]
            param_value = str(match_text[idx + 1 :])

            # strip leading/trailing newline from value
            if param_value.startswith("\n"):
                param_value = param_value[1:]
            if param_value.endswith("\n"):
                param_value = param_value[:-1]

            param_dict[param_name] = self._convert_param_value(param_value)

        try:
            return FunctionCall(
                name=function_name,
                arguments=json.dumps(param_dict, ensure_ascii=False),
            )
        except Exception:
            logger.error("Failed to build FunctionCall for Qwen3-Coder.")
            return None

    def _get_function_calls(self, model_output: str) -> list[str]:
        # Find all tool calls
        matched_ranges = self.tool_call_regex.findall(model_output)
        raw_tool_calls = [match[0] if match[0] else match[1] for match in matched_ranges]

        # Back-off if no tool_call tags found
        if len(raw_tool_calls) == 0:
            raw_tool_calls = [model_output]

        raw_function_calls: list[str] = []
        for tool_call in raw_tool_calls:
            raw_function_calls.extend(self.tool_call_function_regex.findall(tool_call))

        function_calls = [match[0] if match[0] else match[1] for match in raw_function_calls]
        return function_calls

    @rollout_trace_op
    async def extract_tool_calls(self, responses_ids: list[int]) -> tuple[str, list[FunctionCall]]:
        loop = asyncio.get_running_loop()
        text = await loop.run_in_executor(None, self.tokenizer.decode, responses_ids)

        # Quick check to avoid unnecessary processing
        if self.tool_call_prefix not in text and self.tool_call_start_token not in text:
            return text, []

        try:
            function_calls = self._get_function_calls(text)
            if len(function_calls) == 0:
                return text, []

            tool_calls: list[FunctionCall] = []
            for function_call_str in function_calls:
                parsed = self._parse_xml_function_call(function_call_str)
                if parsed is not None:
                    tool_calls.append(parsed)

            # Extract content before tool calls (similar to vLLM impl)
            idx = text.find(self.tool_call_start_token)
            if idx < 0:
                idx = text.find(self.tool_call_prefix)
            content = text[:idx] if idx >= 0 else text

            return content, tool_calls
        except Exception:
            logger.exception("Error in extracting Qwen3-Coder tool calls.")
            return text, []

@ToolParser.register("gptoss")
class GptOssToolParser(ToolParser):
    """Parser for gptoss harmony-style tool calls.

    Supports patterns like:
    - <|channel|>commentary to=namespace.func<|constrain|>json<|message|>{...}<|call|>
    - <|channel|>commentary<|message|>...<|end|>
    - <|channel|>final<|message|>...<|return|>
    """

    def __init__(self, tokenizer) -> None:
        super().__init__(tokenizer)

        self._start_assistant_regex = re.compile(r"<\|start\|>assistant(?!\w)")

        # Full function call: commentary with to=..., json constraint, message payload, ended by <|call|>
        self._function_call_regex = re.compile(
            r"(?:<\|start\|>assistant)?<\|channel\|>commentary\s+to=([a-zA-Z_][a-zA-Z0-9_]*(?:\.[a-zA-Z_][a-zA-Z0-9_]*)*)\s*"
            r"<\|constrain\|>json<\|message\|>(.*?)<\|call\|>(?:commentary)?",
            re.DOTALL,
        )

        # Commentary without tool call
        self._commentary_plain_regex = re.compile(
            r"<\|channel\|>commentary<\|message\|>(.*?)<\|end\|>",
            re.DOTALL,
        )

        # Any commentary (with or without to=) to help compute non-overlapping text blocks
        self._commentary_any_regex = re.compile(
            r"<\|channel\|>commentary(?:\s+to=[^<]*)?<\|message\|>(.*?)(?:<\|end\|>|<\|call\|>)",
            re.DOTALL,
        )

        # Final channel for concluding text
        self._final_regex = re.compile(
            r"<\|channel\|>final<\|message\|>(.*?)(?:<\|return\|>|$)",
            re.DOTALL,
        )

    @rollout_trace_op
    async def extract_tool_calls(self, responses_ids: list[int]) -> tuple[str, list[FunctionCall]]:
        loop = asyncio.get_running_loop()
        text = await loop.run_in_executor(None, self.tokenizer.decode, responses_ids)

        # Quick exit if there is no commentary channel at all
        if "<|channel|>commentary" not in text and "<|channel|>final" not in text:
            return text, []

        # Collect tool calls
        function_calls: list[FunctionCall] = []
        processed_ranges: list[tuple[int, int]] = []

        for match in self._function_call_regex.finditer(text):
            processed_ranges.append((match.start(), match.end()))
            full_function_name = match.group(1)
            args_content = match.group(2)

            function_name = (
                full_function_name.split(".")[-1]
                if "." in full_function_name
                else full_function_name
            )

            try:
                arguments = json.loads(args_content) if args_content.strip() else {}
                function_calls.append(
                    FunctionCall(
                        name=function_name,
                        arguments=json.dumps(arguments, ensure_ascii=False),
                    )
                )
            except Exception as e:
                logger.error(f"Failed to decode gptoss tool call: {e}")

        # Aggregate normal text from commentary without to= and final channel
        normal_text_parts: list[str] = []

        # commentary without to=
        for match in self._commentary_any_regex.finditer(text):
            s, e = match.start(), match.end()
            # skip if this range overlaps with a tool call range
            overlaps_tool = any((ps <= s < pe) or (ps < e <= pe) for ps, pe in processed_ranges)
            if overlaps_tool:
                continue
            content = match.group(1)
            if content:
                content = content.strip()
                if content:
                    normal_text_parts.append(content)

        # Remaining tail including possible final channel
        tail_start = max((end for _, end in processed_ranges), default=0)
        if tail_start < len(text):
            remaining_text = text[tail_start:]
        else:
            remaining_text = ""

        # Remove stray <|start|>assistant tokens
        if remaining_text:
            remaining_text = self._start_assistant_regex.sub("", remaining_text)

            final_match = self._final_regex.search(remaining_text)
            if final_match:
                before_final = remaining_text[: final_match.start()].strip()
                final_content = final_match.group(1).strip()
                if before_final:
                    normal_text_parts.append(before_final)
                if final_content:
                    normal_text_parts.append(final_content)
            else:
                rem = remaining_text.strip()
                if rem:
                    normal_text_parts.append(rem)

        content = " ".join(part for part in normal_text_parts if part).strip()
        return content, function_calls
